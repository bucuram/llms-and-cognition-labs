{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab covers:\n",
    "- High-level explanations of the fundamental concepts behind large\n",
    "language models (LLMs)\n",
    "- Insights into the transformer architecture from which LLMs are derived\n",
    "- A plan for building an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before the advent of LLMs, traditional methods excelled at\n",
    "categorization tasks** such as email spam classification and straightforward\n",
    "pattern recognition that could be captured with handcrafted rules or simpler\n",
    "models. \n",
    "\n",
    "However, they typically **underperformed in language tasks** that\n",
    "demanded complex understanding and generation abilities, such as parsing\n",
    "detailed instructions, conducting contextual analysis, or creating coherent and\n",
    "contextually appropriate original text. \n",
    "\n",
    "For example, previous generations of\n",
    "language models could not write an email from a list of keywordsâ€”a task\n",
    "that is trivial for contemporary LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While pre-LLMs NLP models were tipically designed for specific tasks, LLMs are designed to be general-purpose models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An LLM is a neural network designed to process, generate, and respond to human-like text.**\n",
    "\n",
    "The **\"large\"** in large language model refers to both the model's size in terms\n",
    "of parameters and the immense dataset on which it's trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs utilize the **transformer architecture**, which allows them to pay selective attention to different parts of\n",
    "the input when making predictions, making them especially adept at handling\n",
    "the nuances and complexities of human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stages of building LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/w6q2vA2.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pretraining\n",
    "\n",
    "The term \"pre\" in \"pretraining\" refers to the initial phase where a model like\n",
    "an LLM is trained on a large, diverse dataset to develop a broad\n",
    "understanding of language. The pretrained model is often called **base** or **foundation model**.\n",
    "\n",
    "A typical\n",
    "example of such a model is the GPT-3 model. This model is capable of text completion. It also has limited fewshot capabilities, which means it can learn to perform new tasks based on\n",
    "only a few examples instead of needing extensive training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finetuning\n",
    "\n",
    "The pretrained model can be further refined through finetuning, a\n",
    "process where the model is specifically trained on a narrower dataset that is\n",
    "more specific to particular tasks or domains.\n",
    "\n",
    "- Instruction finetuning - the labeled dataset consists of instruction and answer pairs\n",
    "- Finetuning for classification - the labeled dataset consists of text and corresponding labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/EXaitPE.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture consists of two\n",
    "submodules, an **encoder** and a **decoder**.\n",
    "\n",
    "The **encoder** module processes the\n",
    "input text and encodes it into a series of numerical representations or vectors\n",
    "that capture the contextual information of the input.\n",
    "\n",
    "The **decoder**\n",
    "module takes these encoded vectors and generates the output text from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key component of transformers and LLMs is the **self-attention mechanism**\n",
    "(not shown), which allows the model to weigh the importance of different\n",
    "words or tokens in a sequence relative to each other. It enables\n",
    "the model to capture long-range dependencies and contextual relationships\n",
    "within the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT vs. GPT\n",
    "\n",
    "The encoder segment exemplifies BERT-like LLMs, which focus on masked word prediction and are primarily used for tasks like text classification. \n",
    "\n",
    "The decoder segment\n",
    "showcases GPT-like LLMs, designed for generative tasks and producing coherent text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/8kP92gc.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Pretrained Transformer (GPT) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT models are pretrained on a relatively simple next-word prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/ta0NRXz.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT architecture is based on the decoder module of the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of **autoregressive model**. \n",
    "\n",
    "Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves coherence of the resulting text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/hHhATvY.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/SfY0VaV.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stage 1** - data processing, the attention mechanism, building the model\n",
    "- **Stage 2** - pretraining and evaluation\n",
    "- **Stage 3** - finetuning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
