{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# Implementing a GPT model from Scratch To Generate Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbbe1e",
   "metadata": {},
   "source": [
    "This lab covers\n",
    "- Coding a GPT-like large language model (LLM) that can be trained to generate human-like text\n",
    "- Normalizing layer activations to stabilize neural network training\n",
    "- Adding shortcut connections in deep neural networks to train models more effectively\n",
    "- Implementing transformer blocks to create GPT models of various sizes\n",
    "- Computing the number of parameters and storage requirements of GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "torch version: 2.0.1\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- In this lab, we implement a GPT-like LLM architecture; the next lab will focus on training this LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 1 Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- GPT and Llama generate words sequentially and are based on the decoder part of the original transformer architecture\n",
    "- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n",
    "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n",
    "- However, despite model size, the model architecture is not complicated, since many of its components are repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- In previous labs, we used small embedding dimensions for token inputs and outputs for ease of illustration\n",
    "- In this lab, we consider embedding and model sizes akin to a small GPT-2 model\n",
    "- We'll specifically code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (note that the initial report lists it as 117M parameters, but this was later corrected in the model weight repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c65191",
   "metadata": {},
   "source": [
    "- In the context of deep learning and LLMs like GPT, the term ``parameters`` refers to the trainable weights of the model. \n",
    "- These weights are essentially the internal variables of the model that are adjusted and optimized\n",
    "during the training process to minimize a specific loss function. This optimization allows the model to learn from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24731809",
   "metadata": {},
   "source": [
    "- For example, in a neural network layer that is represented by a 2,048x2,048- dimensional matrix (or tensor) of weights, each element of this matrix is a parameter. Since there are 2,048 rows and 2,048 columns, the total number of\n",
    "parameters in this layer is 2,048 multiplied by 2,048, which equals 4,194,304\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- Configuration details for the 124 million parameter GPT-2 model include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "\n",
    "- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer discussed in lab 1\n",
    "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered in lab 1\n",
    "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
    "- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented in lab 2\n",
    "- `\"n_layers\"` is the number of transformer blocks within the model, which we'll implement in upcoming sections\n",
    "- `\"drop_rate\"` is the dropout mechanism's intensity, discussed in lab 2; 0.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
    "- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism (from lab 2) should include a bias vector when computing query (Q), key (K), and value (V) tensors; we'll disable this option, which is standard practice in modern LLMs; however, we'll revisit this later when loading pretrained GPT-2 weights from OpenAI into our reimplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e03693",
   "metadata": {},
   "source": [
    "- Using the configuration above, we will start by implementing a GPT placeholder architecture (`DummyGPTModel`) in this section, as shown in the figure below. \n",
    "- This will provide us with a big-picture view of how everything fits together and what other components we need to code in the upcoming\n",
    "sections to assemble the full GPT model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9e061",
   "metadata": {},
   "source": [
    "- We will start with step 1, a placeholder GPT backbone we call `DummyGPTModel`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8f8dc",
   "metadata": {},
   "source": [
    "- The `DummyGPTModel` class defines a simplified version of a GPT-like model using PyTorch's neural network module (nn.Module). \n",
    "- The model architecture consists of token and positional embeddings, dropout, a series of transformer blocks (`DummyTransformerBlock`), a final layer normalization (`DummyLayerNorm`), and a linear output layer (out_head). \n",
    "- The configuration is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad073973",
   "metadata": {},
   "source": [
    "- The `forward` method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies dropout,\n",
    "processes the data through the transformer blocks, applies normalization, and finally produces logits with the linear output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e291412",
   "metadata": {},
   "source": [
    "- For now, we have used placeholders (`DummyLayerNorm` and `DummyTransformerBlock`) for the transformer block and layer normalization, which we will develop in later\n",
    "sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7870caf",
   "metadata": {},
   "source": [
    "- Next, we will prepare the input data and initialize a new GPT model to illustrate its usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e8ab-20ca-4100-b9b9-50d9bdee33be",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20be51",
   "metadata": {},
   "source": [
    "- We tokenize a batch consisting\n",
    "of two text inputs for the GPT model using the tiktoken tokenizer introduced\n",
    "in lab 1 and print the resulting token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794b6b6c-d36f-411e-a7db-8ac566a87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa90e2d",
   "metadata": {},
   "source": [
    "- Next, we initialize a new 124 million parameter ``DummyGPTModel`` instance and\n",
    "feed it the tokenized batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8068eb",
   "metadata": {},
   "source": [
    "- The model outputs are commonly referred to as logits\n",
    "- The output tensor has two rows corresponding to the two text samples. Each\n",
    "text sample consists of 4 tokens; each token is a 50,257-dimensional vector,\n",
    "which matches the size of the tokenizer's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 2 Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- Training deep neural networks with many layers can sometimes prove\n",
    "challenging due to issues like vanishing or exploding gradients. These issues\n",
    "lead to unstable training dynamics and make it difficult for the network to\n",
    "effectively adjust its weights, which means the learning process struggles to\n",
    "find a set of parameters (weights) for the neural network that minimizes the\n",
    "loss function.\n",
    "- Layer normalization, also known as LayerNorm ([Ba et al. 2016](https://arxiv.org/abs/1607.06450)), centers the activations of a neural network layer around a mean of 0 and normalizes their variance to 1\n",
    "- This stabilizes training and enables faster convergence to effective weights\n",
    "- Layer normalization is applied both before and after the multi-head attention module within the transformer block, which we will implement later; it's also applied before the final output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- Let's see how layer normalization works by passing a small input sample through a simple neural network layer, recreating the example shown in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69eca5",
   "metadata": {},
   "source": [
    "- We implement a neural network layer with 5 inputs and 6 outputs that\n",
    "we apply to two input examples.\n",
    "- The neural network layer consists of a Linear layer followed\n",
    "by a non-linear activation function, ReLU,\n",
    "which is a standard activation function in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1b463-dc3f-44ac-9cdb-9d5b6f64eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68d80e",
   "metadata": {},
   "source": [
    "- The output is a tensor, where the first row lists the layer outputs for\n",
    "the first input and the second row lists the layer outputs for the second row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- Let's compute the mean and variance for each of the 2 inputs above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043632d",
   "metadata": {},
   "source": [
    "- The first row in the mean tensor above contains the mean value for the first\n",
    "input row, and the second output row contains the mean for the second input\n",
    "row.\n",
    "- Using ``keepdim=True`` in ``mean`` or ``variance`` calculation ensures\n",
    "that the output tensor retains the same shape as the input tensor, even though\n",
    "the operation reduces the tensor along the dimension specified via dim. For\n",
    "instance, without ``keepdim=True``, the returned mean tensor would be a 2-\n",
    "dimensional vector ``[0.1324, 0.2170]`` instead of a 2×1-dimensional matrix\n",
    "``[[0.1324], [0.2170]]``.\n",
    "- The ``dim`` parameter specifies the dimension along which the calculation of the\n",
    "statistic (here, mean or variance) should be performed in a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- The normalization is applied to each of the two inputs (rows) independently; using ``dim=-1`` applies the calculation across the last dimension (in this case, the feature dimension) instead of the row dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528971e0",
   "metadata": {},
   "source": [
    "- For a 2D tensor (like a matrix), using ``dim=-1`` for\n",
    "operations such as mean or variance calculation is the same as using ``dim=1``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "source": [
    "- Let's apply layer normalization to the layer outputs we obtained earlier.\n",
    "- Subtracting the mean and dividing by the square-root of the variance (standard deviation) centers the inputs to have a mean of 0 and a variance of 1 across the column (feature) dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0f40a",
   "metadata": {},
   "source": [
    "- As we can see based on the results, the normalized layer outputs, which now\n",
    "also contain negative values, have zero mean and a variance of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b90c-7156-4979-9a79-ce1fb92969c1",
   "metadata": {},
   "source": [
    "- To improve readability, we can disable PyTorch's scientific notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e06c34b-c68a-4b36-afbe-b30eda4eca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "- Above, we normalized the features of each input\n",
    "- Now, using the same idea, we can implement a `LayerNorm` class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b20288",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3333a305-aa3d-460a-bcce-b80662d464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3908-7544-4808-b8cb-5d0a55bcca72",
   "metadata": {},
   "source": [
    "**Scale and shift**\n",
    "\n",
    "- Note that in addition to performing the normalization by subtracting the mean and dividing by the variance, we added two trainable parameters, a `scale` and a `shift` parameter\n",
    "- The initial `scale` (multiplying by 1) and `shift` (adding 0) values don't have any effect; however, `scale` and `shift` are trainable parameters that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task\n",
    "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing\n",
    "- Note that we also add a smaller value (`eps`) before computing the square root of the variance; this is to avoid division-by-zero errors if the variance is 0\n",
    "\n",
    "**Biased variance**\n",
    "- In the variance calculation above, setting `unbiased=False` means using the formula $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$ to compute the variance where n is the sample size (here, the number of features or columns); this formula does not include Bessel's correction (which uses `n-1` in the denominator), thus providing a biased estimate of the variance \n",
    "- For LLMs, where the embedding dimension `n` is very large, the difference between using n and `n-1`\n",
    " is negligible\n",
    "- However, GPT-2 was trained with a biased variance in the normalization layers, which is why we also adopted this setting for compatibility reasons with the pretrained weights that we will load in later chapters\n",
    "\n",
    "- Let's now try out `LayerNorm` in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b1000a-e613-4b43-bd90-e54deed8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c12de2-1cab-46e0-a099-e2e470353bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11190e7d-8c29-4115-824a-e03702f9dd54",
   "metadata": {},
   "source": [
    "## 3 Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585dfb-f21e-40e5-973f-2f63ad5cb169",
   "metadata": {},
   "source": [
    "- In this section, we implement a small neural network submodule that is used as part of the transformer block in LLMs\n",
    "- We start with the activation function\n",
    "- In deep learning, ReLU (Rectified Linear Unit) activation functions are commonly used due to their simplicity and effectiveness in various neural network architectures\n",
    "- In LLMs, various other types of activation functions are used beyond the traditional ReLU; two notable examples are GELU (Gaussian Error Linear Unit) and SwiGLU (Swish-Gated Linear Unit)\n",
    "- GELU and SwiGLU are more complex, smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively, offering better performance for deep learning models, unlike the simpler, piecewise linear function of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482ce7-e493-4bfc-a820-3ea99f564ebc",
   "metadata": {},
   "source": [
    "- GELU ([Hendrycks and Gimpel 2016](https://arxiv.org/abs/1606.08415)) can be implemented in several ways; the exact version is defined as GELU(x)=x⋅Φ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian distribution.\n",
    "- In practice, it's common to implement a computationally cheaper approximation: $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$ (the original GPT-2 model was also trained with this approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bdcaf",
   "metadata": {},
   "source": [
    "- We can implement this function as PyTorch module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84694b7-95f3-4323-b6d6-0a73df278e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118df81",
   "metadata": {},
   "source": [
    "- **Note:** If the code below is not working, and you are using torch version: 2.0.1, please downgrade your numpy version to 1.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc5487d2-2576-4118-80a7-56c4caac2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- As we can see, ReLU is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero\n",
    "- GELU is a smooth, non-linear function that approximates ReLU but with a non-zero gradient for negative values (except at approximately -0.75)\n",
    "- The smoothness of GELU, as shown in the figure above, can lead to better\n",
    "optimization properties during training, as it allows for more nuanced\n",
    "adjustments to the model's parameters. In contrast, ReLU has a sharp cornerat zero, which can sometimes make optimization harder, especially in\n",
    "networks that are very deep or have complex architectures. \n",
    "- Moreover, unlike\n",
    "RELU, which outputs zero for any negative input, GELU allows for a small,\n",
    "non-zero output for negative values. This characteristic means that during the\n",
    "training process, neurons that receive negative input can still contribute to the\n",
    "learning process, albeit to a lesser extent than positive inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e20ec",
   "metadata": {},
   "source": [
    "- Next, let's implement the small neural network module, `FeedForward`, that we will be using in the LLM's transformer block later\n",
    "- The ``FeedForward`` module is a small\n",
    "neural network consisting of two Linear layers and a GELU activation\n",
    "function. In the 124 million parameter GPT model, it receives the input\n",
    "batches with tokens that have an embedding size of 768 each via the\n",
    "``GPT_CONFIG_124M`` dictionary where ``GPT_CONFIG_124M[\"emb_dim\"] = 768``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27875c2e",
   "metadata": {},
   "source": [
    "- The figure below shows how the embedding size is manipulated inside this small\n",
    "feed forward neural network when we pass it some inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/09.webp?12\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cfe67",
   "metadata": {},
   "source": [
    "- Following the example in the figure, let's initialize a new ``FeedForward``\n",
    "module with a token embedding size of 768 and feed it a batch input with 2\n",
    "samples and 3 tokens each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928e7f7c-d0b1-499f-8d07-4cadb428a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d5b6c",
   "metadata": {},
   "source": [
    "- The ``FeedForward`` module we implemented in this section plays a crucial role\n",
    "in enhancing the model's ability to learn from and generalize the data.\n",
    "- Although the input and output dimensions of this module are the same, it\n",
    "internally expands the embedding dimension into a higher-dimensional space\n",
    "through the first linear layer as illustrated in figure below. \n",
    "- This expansion is\n",
    "followed by a non-linear GELU activation, and then a contraction back to the\n",
    "original dimension with the second linear transformation. \n",
    "- Such a design\n",
    "allows for the exploration of a richer representation space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4 Adding shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da2a50-04f4-4388-af23-ad32e405a972",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae416c-821e-4bfa-a741-8af4ba5db00e",
   "metadata": {},
   "source": [
    "- Next, let's talk about the concept behind shortcut connections, also called skip or residual connections\n",
    "- Originally, **shortcut connections were proposed in deep networks for computer vision (residual networks) to mitigate vanishing gradient problems**\n",
    "- A shortcut connection creates an alternative shorter path for the gradient to flow through the network\n",
    "- This is achieved by adding the output of one layer to the output of a later layer, usually skipping one or more layers in between\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- Let's implement a deep neural network with 5 layers, each consisting of\n",
    "a ``Linear layer`` and a ``GELU activation function``. \n",
    "- In the forward pass, we\n",
    "iteratively pass the input through the layers and optionally add the shortcut\n",
    "connections depicted in the figure above if the ``self.use_shortcut`` attribute is set\n",
    "to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feffb6",
   "metadata": {},
   "source": [
    "- We specified a loss function that computes how close the\n",
    "model output and a user-specified target (here, for simplicity, the value 0)\n",
    "are. Then, when calling ``loss.backward()``, PyTorch computes the loss\n",
    "gradient for each layer in the model. We can iterate through the weight\n",
    "parameters via ``model.named_parameters()``.\n",
    "- The ``.backward()`` method is a convenient method in PyTorch that\n",
    "computes loss gradients, which are required during model training, without\n",
    "implementing the math for the gradient calculation ourselves, thereby making\n",
    "working with deep neural networks much more accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- Let's use the code to first initialize a neural network without shortcut\n",
    "connections. Here, each layer will be initialized such that it accepts an\n",
    "example with 3 input values and returns 3 output values. The last layer\n",
    "returns a single output value\n",
    "- Let's print the gradient values first **without** shortcut connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768bac1",
   "metadata": {},
   "source": [
    "- As we can see based on the output of the ``print_gradients`` function, the\n",
    "gradients become smaller as we progress from the last layer (``layers.4``) to the\n",
    "first layer (``layers.0``), which is a phenomenon called the **vanishing gradient**\n",
    "problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- Next, let's print the gradient values **with** shortcut connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight has gradient mean of 0.20694111287593842\n",
      "layers.2.0.weight has gradient mean of 0.3289700150489807\n",
      "layers.3.0.weight has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight has gradient mean of 1.3258544206619263\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff783a-46f0-49c5-a7a9-26a525764b6e",
   "metadata": {},
   "source": [
    "- As we can see, based on the output, the last layer (``layers.4``) still has a larger\n",
    "gradient than the other layers. However, the gradient value stabilizes as we\n",
    "progress towards the first layer (``layers.0``) and doesn't shrink to a\n",
    "vanishingly small value.\n",
    "- Shortcut connections prevent the gradients from vanishing in the early layers\n",
    "- We will use this concept of a shortcut connection next when we implement a transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935a5df",
   "metadata": {},
   "source": [
    "- In conclusion, shortcut connections are important for overcoming the\n",
    "limitations posed by the vanishing gradient problem in deep neural networks.\n",
    "- Shortcut connections are a core building block of very large models such as\n",
    "LLMs, and they will help facilitate more effective training by ensuring\n",
    "consistent gradient flow across layers when we train the GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae578ca-e564-42cf-8635-a2267047cdff",
   "metadata": {},
   "source": [
    "## 5 Connecting attention and linear layers in a transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daac6f-6545-4258-8f2d-f45a7394f429",
   "metadata": {},
   "source": [
    "- In this section, we now combine the previous concepts into a so-called transformer block\n",
    "- A transformer block combines the causal multi-head attention module from the previous chapter with the linear layers, the feed forward neural network we implemented in an earlier section\n",
    "- In addition, the transformer block also uses dropout and shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c3fc0",
   "metadata": {},
   "source": [
    "- When a transformer block processes an input sequence, each element in the\n",
    "sequence (for example, a word or subword token) is represented by a fixed-\n",
    "size vector (in our case, 768 dimensions). \n",
    "- The operations\n",
    "within the transformer block, including multi-head attention and feed forward\n",
    "layers, are designed to transform these vectors in a way that preserves their\n",
    "dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f973ba",
   "metadata": {},
   "source": [
    "- The idea is that the self-attention mechanism in the multi-head attention\n",
    "block identifies and analyzes relationships between elements in the input\n",
    "sequence. \n",
    "- In contrast, the feed forward network modifies the data\n",
    "individually at each position. \n",
    "- This combination not only enables a more\n",
    "nuanced understanding and processing of the input but also enhances the\n",
    "model's overall capacity for handling complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633fd50c",
   "metadata": {},
   "source": [
    "- But first, let's define again the ``MultiHeadAttention`` class from lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01859185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bee51",
   "metadata": {},
   "source": [
    "- And now, let's define our `TransformerBlock` that uses `MultiHeadAttention` and a feed forward\n",
    "network (``FeedForward``), both configured based on a provided configuration\n",
    "dictionary (cfg), such as GPT_CONFIG_124M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0766798",
   "metadata": {},
   "source": [
    "- Layer normalization (``LayerNorm``) is applied before each of these two\n",
    "components, and dropout is applied after them to regularize the model and\n",
    "prevent overfitting. This is also known as ``Pre-LayerNorm``. Older\n",
    "architectures, such as the original transformer model, applied layer\n",
    "normalization after the self-attention and feed-forward networks instead,\n",
    "known as ``Post-LayerNor``m, which often leads to **worse** training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3a65d",
   "metadata": {},
   "source": [
    "- The class also implements the ``forward`` pass, where each component is\n",
    "followed by a shortcut connection that adds the input of the block to its\n",
    "output. This critical feature helps gradients flow through the network during\n",
    "training and improves the learning of deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d375-87bd-4153-9040-63a1e6a2b7cb",
   "metadata": {},
   "source": [
    "- Suppose we have 2 input samples with 6 tokens each, where each token is a 768-dimensional embedding vector; then this transformer block applies self-attention, followed by linear layers, to produce an output of similar size\n",
    "- The preservation of shape throughout the transformer block architecture is\n",
    "not incidental but a crucial aspect of its design. This design enables its\n",
    "effective application across a wide range of sequence-to-sequence tasks,\n",
    "where each output vector directly corresponds to an input vector, maintaining\n",
    "a one-to-one relationship. \n",
    "- The output is a context vector that\n",
    "encapsulates information from the entire input sequence, as we learned in\n",
    "lab 2. This means that while the physical dimensions of the sequence\n",
    "(length and feature size) remain unchanged as it passes through the\n",
    "transformer block, the content of each output vector is re-encoded to integrate\n",
    "contextual information from across the entire input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 6 Coding the GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4ee4-cf23-4583-b1fd-317abb4fcd13",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7ef60",
   "metadata": {},
   "source": [
    "- We are almost there: now let's plug in the transformer block into the architecture we coded at the very beginning of this lab so that we obtain a usable GPT architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fe02f",
   "metadata": {},
   "source": [
    "- We are now replacing the ``DummyTransformerBlock`` and\n",
    "``DummyLayerNorm`` placeholders with the real ``TransformerBlock`` and\n",
    "``LayerNorm`` classes to assemble a fully working\n",
    "version of the original 124 million parameter version of GPT-2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
   "metadata": {},
   "source": [
    "\n",
    "- Note that the transformer block is repeated multiple times; in the case of the smallest 124M GPT-2 model, we repeat it 12 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- The corresponding code implementation, where `cfg[\"n_layers\"] = 12`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c498d",
   "metadata": {},
   "source": [
    "- Thanks to the ``TransformerBlock`` class we implemented earlier, the\n",
    "``GPTModel`` class is relatively small and compact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339f896",
   "metadata": {},
   "source": [
    "\n",
    "- The ``__init__`` constructor of this ``GPTModel`` class initializes the token and\n",
    "positional embedding layers using the configurations passed in via a Python\n",
    "dictionary, ``cfg``. These embedding layers are responsible for converting input\n",
    "token indices into dense vectors and adding positional information.\n",
    "- Next, the ``__init__`` method creates a sequential stack of ``TransformerBlock``\n",
    "modules equal to the number of layers specified in cfg. Following the\n",
    "transformer blocks, a ``LayerNorm`` layer is applied, standardizing the outputs\n",
    "from the transformer blocks to stabilize the learning process.\n",
    "- Finally, a linear\n",
    "output head without bias is defined, which projects the transformer's output\n",
    "into the vocabulary space of the tokenizer to generate logits for each token in\n",
    "the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bcae6",
   "metadata": {},
   "source": [
    "- The ``forward`` method takes a batch of input token indices, computes their\n",
    "embeddings, applies the positional embeddings, passes the sequence through\n",
    "the transformer blocks, normalizes the final output, and then computes the\n",
    "logits, representing the next token's unnormalized probabilities. We will\n",
    "convert these logits into tokens and text outputs in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
   "metadata": {},
   "source": [
    "- Using the configuration of the 124M parameter model, we can now instantiate this GPT model with random initial weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c79c74",
   "metadata": {},
   "source": [
    "- As we can see, the output tensor has the shape ``[2, 4, 50257]``, since we\n",
    "passed in 2 input texts with 4 tokens each. The last dimension, 50,257,\n",
    "corresponds to the vocabulary size of the tokenizer. In the next section, we\n",
    "will see how to convert each of these 50,257-dimensional output vectors back\n",
    "into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616e7a-568b-4921-af29-bd3f4683cd2e",
   "metadata": {},
   "source": [
    "- We will train this model in the next lab\n",
    "- However, a quick note about its size: we previously referred to it as a 124M parameter model; we can double check this number as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc033f5",
   "metadata": {},
   "source": [
    "Using the ``numel()`` method, short for \"number of elements,\" we can collect\n",
    "the total number of parameters in the model's parameter tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660",
   "metadata": {},
   "source": [
    "- As we see above, this model has 163M, not 124M parameters; why?\n",
    "- In the original GPT-2 paper, the researchers applied weight tying, which means that they reused the token embedding layer (`tok_emb`) as the output layer, which means setting `self.out_head.weight = self.tok_emb.weight`\n",
    "- The token embedding layer projects the 50,257-dimensional one-hot encoded input tokens to a 768-dimensional embedding representation\n",
    "- The output layer projects 768-dimensional embeddings back into a 50,257-dimensional representation so that we can convert these back into words (more about that in the next section)\n",
    "- So, the embedding and output layer have the same number of weight parameters, as we can see based on the shape of their weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- In the original GPT-2 paper, the researchers reused the token embedding matrix as an output matrix\n",
    "- Correspondingly, if we subtracted the number of parameters of the output layer, we'd get a 124M parameter model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a22e02-50d3-48b3-a4e0-d9863343c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b03f80-b94c-46e7-9d42-d0df399ff3db",
   "metadata": {},
   "source": [
    "- In practice, it is easier to train the model without weight-tying, which is why we didn't implement it here\n",
    "- Lastly, we can compute the memory requirements of the model as follows, which can be a helpful reference point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- Exercise: you can try the following other configurations, which are referenced in the [GPT-2 paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C), as well.\n",
    "\n",
    "    - **GPT2-small** (the 124M configuration we already implemented):\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "    \n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "    \n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 7 Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- LLMs like the GPT model we implemented above are used to generate one word at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
   "metadata": {},
   "source": [
    "- The following `generate_text_simple` function implements greedy decoding, which is a simple and fast method to generate text\n",
    "- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n",
    "- The figure below depicts how the GPT model, given an input context, generates the next word token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467673f",
   "metadata": {},
   "source": [
    "- The process by which a GPT model goes from output tensors to generated\n",
    "text involves several steps: decoding the output tensors, selecting tokens based on a probability\n",
    "distribution, and converting these tokens into human-readable text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ac701",
   "metadata": {},
   "source": [
    "- The next-token generation process detailed in the figure amove illustrates a single\n",
    "step where the GPT model generates the next token given its input.\n",
    "- In each step, the model outputs a matrix with vectors representing potential\n",
    "next tokens. \n",
    "- The vector corresponding to the next token is extracted and\n",
    "converted into a probability distribution via the softmax function. \n",
    "- Within the\n",
    "vector containing the resulting probability scores, the index of the highest\n",
    "value is located, which translates to the token ID. \n",
    "- This token ID is then\n",
    "decoded back into text, producing the next token in the sequence. \n",
    "- Finally, this\n",
    "token is appended to the previous inputs, forming a new input sequence for\n",
    "the subsequent iteration. \n",
    "- This step-by-step process enables the model to\n",
    "generate text sequentially, building coherent phrases and sentences from the\n",
    "initial input context.\n",
    "- In practice, we repeat this process over many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff308f",
   "metadata": {},
   "source": [
    "- We can implement the token-generation process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73193e1",
   "metadata": {},
   "source": [
    "- Above is a a simple implementation of a\n",
    "generative loop for a language model using PyTorch. It iterates for a\n",
    "specified number of new tokens to be generated, crops the current context to\n",
    "fit the model's maximum context size, computes predictions and then selects\n",
    "the next token based on the highest probability prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7316e",
   "metadata": {},
   "source": [
    "- In the ``generate_text_simple function``, we use a\n",
    "``softmax`` function to convert the logits into a probability distribution from\n",
    "which we identify the position with the highest value via ``torch.argmax``. \n",
    "- The\n",
    "``softmax`` function is monotonic, meaning it preserves the order of its inputs\n",
    "when transformed into outputs. So, in practice, the ``softmax`` step is redundant\n",
    "since the position with the highest score in the softmax output tensor is the\n",
    "same position in the logit tensor. \n",
    "- In other words, we could apply the\n",
    "``torch.argmax`` function to the logits tensor directly and get identical results.\n",
    "However, we coded the conversion to illustrate the full process of\n",
    "transforming logits to probabilities, which can add additional intuition, such\n",
    "as that the model generates the most likely next token, which is known as\n",
    "``greedy decoding``.\n",
    "- In the next lab, when we will implement the GPT training code, **we will\n",
    "also introduce additional sampling techniques where we modify the softmax\n",
    "outputs such that the model doesn't always select the most likely token, which\n",
    "introduces variability and creativity in the generated text**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- This process of generating one token ID at a time and appending it to the\n",
    "context using the generate_text_simple function is further illustrated below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5a065",
   "metadata": {},
   "source": [
    "- We generate the token IDs in an iterative fashion.\n",
    "For instance, in iteration 1, the model is provided with the tokens\n",
    "corresponding to \"Hello , I am\", predicts the next token (with ID 257, which\n",
    "is \"a\"), and appends it to the input. This process is repeated until the model\n",
    "produces the complete sentence \"Hello, I am a model ready to help.\" after six\n",
    "iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- Let's now try out the ``generate_text_simple`` function with the \"Hello, I\n",
    "am\" context as model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dac2b1",
   "metadata": {},
   "source": [
    "- First, we encode the input context into token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36337d",
   "metadata": {},
   "source": [
    "- Next, we put the model into ``.eval()`` mode, which disables random\n",
    "components like dropout, which are only used during training, and use the\n",
    "``generate_text_simple`` function on the encoded input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- Remove batch dimension and convert back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894003-51f6-4ccc-996f-3b9c7d5a1d70",
   "metadata": {},
   "source": [
    "- Note that the model is untrained; hence the random output texts above\n",
    "- We will train the model in the next lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
